{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d50cd29",
   "metadata": {
    "_cell_guid": "530d7a43-2a89-4e06-b0e5-6826b53c29a3",
    "_kg_hide-input": true,
    "_uuid": "9b365e69-f3f3-4122-8c69-fcd09ca952b9",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:29.656549Z",
     "iopub.status.busy": "2022-01-13T07:10:29.654915Z",
     "iopub.status.idle": "2022-01-13T07:10:30.779580Z",
     "shell.execute_reply": "2022-01-13T07:10:30.778749Z",
     "shell.execute_reply.started": "2022-01-12T09:44:34.249869Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.14582,
     "end_time": "2022-01-13T07:10:30.779741",
     "exception": false,
     "start_time": "2022-01-13T07:10:29.633921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date \n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "\n",
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87819c62",
   "metadata": {
    "_cell_guid": "7532b09c-7710-4666-a4eb-ccead072f157",
    "_uuid": "0e0cf66f-20f8-4339-ac7e-37207b3f119f",
    "papermill": {
     "duration": 0.015966,
     "end_time": "2022-01-13T07:10:30.815022",
     "exception": false,
     "start_time": "2022-01-13T07:10:30.799056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "<div>    \n",
    "    <div style = \"float:left; width:55%; overflow:hidden;\">        \n",
    "        <br><br><br><br>        \n",
    "        <span style = \"float:right;\">\n",
    "        <h2>Time Series Transformers</h2>\n",
    "        <p>How to use Transformers for Time-series data? 🤔</p>\n",
    "        <br>\n",
    "        <b></b>\n",
    "        <b>\n",
    "        - 🌎 <a href=\"https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/300195\">Discussion Thread</a>\n",
    "        <br>\n",
    "        - 🇰 <a href=\"https://www.kaggle.com/c/tabular-playground-series-jan-2022\">The Competition</a>\n",
    "        </b>            \n",
    "        </span>\n",
    "    </div>\n",
    "    <div style=\"float:right; width:35%; max-height:300px; overflow: hidden;\">        \n",
    "        <img src=\"https://i.ibb.co/5hrW6cC/Robotics.gif\" style = \"max-height: 300px;\">         \n",
    "    </div>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24da6fb",
   "metadata": {
    "_cell_guid": "780449f0-680e-466c-95aa-d0ac8949ae92",
    "_uuid": "77931820-c005-4279-98d9-1565a1428ee0",
    "papermill": {
     "duration": 0.015625,
     "end_time": "2022-01-13T07:10:30.846747",
     "exception": false,
     "start_time": "2022-01-13T07:10:30.831122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "_____\n",
    "\n",
    "<br>\n",
    "\n",
    "## Training Notebok [Here](https://www.kaggle.com/yamqwe/tutorial-time-series-transformer-time2vec)\n",
    "<br>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440aad6d",
   "metadata": {
    "_cell_guid": "3d83f4c2-725f-4c48-b535-3c028518c253",
    "_uuid": "de2d8abf-14e2-4cca-b089-9f116785c6d9",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:30.884220Z",
     "iopub.status.busy": "2022-01-13T07:10:30.883557Z",
     "iopub.status.idle": "2022-01-13T07:10:30.946822Z",
     "shell.execute_reply": "2022-01-13T07:10:30.946372Z",
     "shell.execute_reply.started": "2022-01-12T09:44:35.369999Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.084272,
     "end_time": "2022-01-13T07:10:30.946948",
     "exception": false,
     "start_time": "2022-01-13T07:10:30.862676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil.easter as easter\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aff0e28",
   "metadata": {
    "_cell_guid": "5c5166a8-1e95-4c09-85ae-860a936ae891",
    "_uuid": "99ea7174-2835-4834-aedd-4e8c3a5dac8f",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:30.985493Z",
     "iopub.status.busy": "2022-01-13T07:10:30.984988Z",
     "iopub.status.idle": "2022-01-13T07:10:31.079522Z",
     "shell.execute_reply": "2022-01-13T07:10:31.080081Z",
     "shell.execute_reply.started": "2022-01-12T09:44:35.440101Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.11628,
     "end_time": "2022-01-13T07:10:31.080237",
     "exception": false,
     "start_time": "2022-01-13T07:10:30.963957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Mug</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Finland</td>\n",
       "      <td>KaggleMart</td>\n",
       "      <td>Kaggle Hat</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id       date  country       store     product  num_sold\n",
       "date                                                                    \n",
       "2015-01-01       0 2015-01-01  Finland  KaggleMart  Kaggle Mug       329\n",
       "2015-01-01       1 2015-01-01  Finland  KaggleMart  Kaggle Hat       520"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR = 1e-05\n",
    "WINDOW_SIZE = 60\n",
    "\n",
    "\n",
    "original_train_df = pd.read_csv('./train.csv')\n",
    "original_test_df = pd.read_csv('./test.csv')\n",
    "gdp_df = pd.read_csv('./GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n",
    "gdp_df.set_index('year', inplace=True)\n",
    "\n",
    "for df in [original_train_df, original_test_df]:\n",
    "    df['date'] = pd.to_datetime(df.date)\n",
    "    df.set_index('date', inplace=True, drop=False)\n",
    "original_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5052d3",
   "metadata": {
    "_cell_guid": "b9e6b4f2-37f0-4b07-9136-2622615aecc9",
    "_uuid": "5e745235-d28e-4734-aaf4-c92617f4004a",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:31.125810Z",
     "iopub.status.busy": "2022-01-13T07:10:31.125161Z",
     "iopub.status.idle": "2022-01-13T07:10:31.127379Z",
     "shell.execute_reply": "2022-01-13T07:10:31.127771Z",
     "shell.execute_reply.started": "2022-01-12T09:44:35.567489Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.031262,
     "end_time": "2022-01-13T07:10:31.127921",
     "exception": false,
     "start_time": "2022-01-13T07:10:31.096659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smape_loss(y_true, y_pred): return np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n",
    "\n",
    "def engineer(df):\n",
    "    def get_gdp(row):\n",
    "        country = 'GDP_' + row.country\n",
    "        return gdp_df.loc[row.date.year, country]\n",
    "    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis = 1)), 'wd4': df.date.dt.weekday == 4, 'wd56': df.date.dt.weekday >= 5})\n",
    "    for country in ['Finland', 'Norway']: new_df[country] = df.country == country\n",
    "    new_df['KaggleRama'] = df.store == 'KaggleRama'\n",
    "    for product in ['Kaggle Mug', 'Kaggle Sticker']: new_df[product] = df['product'] == product\n",
    "    dayofyear = df.date.dt.dayofyear\n",
    "    for k in range(1, 20):\n",
    "        new_df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
    "        new_df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
    "        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n",
    "        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n",
    "        new_df[f'sticker_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Sticker']\n",
    "        new_df[f'sticker_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Sticker']\n",
    "    return new_df\n",
    "\n",
    "def rolling_window(df, y = None, window_size = 10):\n",
    "    all_features, all_targets = [], []\n",
    "    for i in range(0, len(df) - window_size):\n",
    "        all_features.append(np.expand_dims(df[i: i + window_size].values, axis = 0))\n",
    "        if y is not None: all_targets.append(np.expand_dims(y[i + window_size], axis = 0))\n",
    "    if y is not None: return np.concatenate(all_features, axis = 0), np.concatenate(all_targets, axis = 0)\n",
    "    else: return np.concatenate(all_features, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc53aa",
   "metadata": {
    "_cell_guid": "b2935397-e4f9-42ad-aa8e-55bfe3d8dc83",
    "_uuid": "99b5d6a0-018e-47f5-96b9-e50b80f1898c",
    "papermill": {
     "duration": 0.016214,
     "end_time": "2022-01-13T07:10:31.160847",
     "exception": false,
     "start_time": "2022-01-13T07:10:31.144633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "> ### See training notebook [here](https://www.kaggle.com/yamqwe/tutorial-time-series-transformer-time2vec)\n",
    "\n",
    "\n",
    "# <span class=\"title-section w3-xxlarge\" id=\"ts_trans\">Time Series Transformers 🤖</span>\n",
    "<hr> \n",
    "\n",
    "**Let's apply a Time-Series transformer with the above set-up!**\n",
    "\n",
    "<!-- ![Multivariate Time Series Transformer Framework](https://storage.googleapis.com/kaggle-markpeng/GoogleBrainVentilator/mvts_transformer_architecture.png) -->\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-markpeng/GoogleBrainVentilator/mvts_transformer_architecture.png\" alt=\"Multivariate Time Series Transformer Framework\" width=\"800\"/>\n",
    "\n",
    "\n",
    "_____\n",
    "**Credits:** Parts of this implementation were insipred / based on: \n",
    "\n",
    "- **George Zerveas _et al._ (2021). \"_A Transformer-based Framework for Multivariate Time Series Representation Learning_,\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21).** - https://arxiv.org/abs/2010.02803\n",
    "- **TensorFlow Transformer** - https://www.kaggle.com/cdeotte/tensorflow-transformer-0-112\n",
    "- **Time2Vec** - https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
    "\n",
    "_____\n",
    "\n",
    "\n",
    "As we all know, transformers are taking over the state-of-the-art title in any field they get into. It was just a matter of time until we got the first papers implementing them for time-series. \n",
    "\n",
    "This rest of the notebook implements a transformer model for learning the representation of a Time-series. \n",
    "It learns to attend both to preceding and succeeding segments in individual features, as well as the inter-dependencies between features.\n",
    "\n",
    "\n",
    "**Differences between Transformers and Time-Series Transformers:**\n",
    "\n",
    "- 1. In the paper, they use batchnorm rather then layernorm, this is because the problem with batchnorm in the first place was variation in input length for NLP. This is the reason for the inferior performance of batch normalization in NLP (i.e., sentences in most tasks) (Shen et al., 2020). \n",
    "\n",
    "- 2. In the paper they use a **Fully learnable positional encodings** rather then a fixed positionl encoding (As the classic BERT is using. It is observed that they perform better for all datasets presented in the paper.\n",
    "\n",
    "- 3. In the paper they have an unsupervised trainin phased using Masked MLM input (sequencial masking starting from left). The model then try to predict all of the input vector but the loss given is only from the masked indices. The reconstruction loss is mse. \n",
    "\n",
    "- 4. In this notebook we apply residual skip connections as it is done on the implementation linked above: We use a constant \"SKIP_CONNECTION_STRENGTH\" and muliply the skip connection by it. This improves performance in practice. \n",
    "\n",
    "- 5. In this notebook we extend the \"learnable embeddings\" and apply Time2Vec ([paper](https://arxiv.org/abs/1907.05321)). This also improves performance in practice. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Architecture & Loss Function\n",
    "\n",
    "The model is trained with keras for about 1,000 epochs using combined training and test sets. The loss function is **MAE loss** trained end2end to the targets. You could play around the hyperparameters for a larger model or using more hand-crafted features as the input.\n",
    "\n",
    "![](https://i.ibb.co/TTDp6kZ/1-TWkm-OC8a-K-2-Tgc-CIm-m-KMg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05eacea",
   "metadata": {
    "_cell_guid": "f24c569e-c481-47a8-9425-d263bccfd5af",
    "_kg_hide-input": true,
    "_uuid": "f858b48c-ee1a-4f4b-b5f1-f2534476af44",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:31.198951Z",
     "iopub.status.busy": "2022-01-13T07:10:31.198365Z",
     "iopub.status.idle": "2022-01-13T07:10:35.831375Z",
     "shell.execute_reply": "2022-01-13T07:10:35.830846Z",
     "shell.execute_reply.started": "2022-01-12T09:44:35.583772Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.654075,
     "end_time": "2022-01-13T07:10:35.831501",
     "exception": false,
     "start_time": "2022-01-13T07:10:31.177426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d8cdfb405cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TF_CPP_MIN_LOG_LEVEL'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'3'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m   \u001b[0mh5py\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdf5_version_tuple\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhdf5_built_version_tuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\version.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mh5\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_h5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\h5.pyx\u001b[0m in \u001b[0;36minit h5py.h5\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "import os; os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd, numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, Reshape, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28978040",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-738abb97652d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#import tensorflow as tf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": [
    "#pip install h5py\n",
    "#import tensorflow as tf\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d85979",
   "metadata": {
    "_cell_guid": "55b6e8a3-5fb5-4b8b-8713-d51648d8ed22",
    "_uuid": "821b2c6f-2f5f-44e0-838d-ec1159ef1393",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:35.874265Z",
     "iopub.status.busy": "2022-01-13T07:10:35.873613Z",
     "iopub.status.idle": "2022-01-13T07:10:35.876715Z",
     "shell.execute_reply": "2022-01-13T07:10:35.876201Z",
     "shell.execute_reply.started": "2022-01-12T09:44:40.159453Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028171,
     "end_time": "2022-01-13T07:10:35.876852",
     "exception": false,
     "start_time": "2022-01-13T07:10:35.848681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, feat_dim, num_heads, ff_dim, rate = 0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
    "        self.ffn = keras.Sequential( [layers.Dense(ff_dim, activation = \"gelu\"), layers.Dense(feat_dim),] )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon = 1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b11a7b",
   "metadata": {
    "_cell_guid": "060d2e50-3651-47fd-93a3-5da6518908d8",
    "_uuid": "c1d02ece-92aa-471c-beea-e3190443d6f0",
    "papermill": {
     "duration": 0.016344,
     "end_time": "2022-01-13T07:10:35.910046",
     "exception": false,
     "start_time": "2022-01-13T07:10:35.893702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "### Time 2 Vec Embeddings\n",
    "\n",
    "Mathematically speaking, implementing Time2Vec is quite easy:\n",
    "\n",
    "![](https://i.ibb.co/yhphBqc/1-E8-Zi-ELOi-Gv-Nu-WCAHX8f-Rw.png)\n",
    "\n",
    "[from https://arxiv.org/pdf/1907.05321.pdf]\n",
    "\n",
    "<br>\n",
    "\n",
    "Where k is the time2vec dimension, tau is a raw time series, F is a periodic activation function, omega and phi are a set of learnable parameters. \n",
    "\n",
    "On this notebook we set F to be a sin function in order to help the algorithm capture periodic behaviors in data. At the same time, the linear term represents the progression of time and can be used for capturing non-periodic patterns in the input that depend on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61adce9",
   "metadata": {
    "_cell_guid": "558645c9-c617-4756-aa28-99f757b5554a",
    "_uuid": "0bbfd2d1-b93e-4c4f-935f-b9f47be6c4a7",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:35.953353Z",
     "iopub.status.busy": "2022-01-13T07:10:35.946537Z",
     "iopub.status.idle": "2022-01-13T07:10:35.958048Z",
     "shell.execute_reply": "2022-01-13T07:10:35.956894Z",
     "shell.execute_reply.started": "2022-01-12T09:44:40.174172Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.031246,
     "end_time": "2022-01-13T07:10:35.958308",
     "exception": false,
     "start_time": "2022-01-13T07:10:35.927062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Time2Vec(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size = 1):\n",
    "        super(Time2Vec, self).__init__(trainable = True, name = 'Time2VecLayer')\n",
    "        self.k = kernel_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.wb = self.add_weight(name = 'wb', shape = (input_shape[1],), initializer = 'uniform', trainable = True)\n",
    "        self.bb = self.add_weight(name = 'bb', shape = (input_shape[1],), initializer = 'uniform', trainable = True)\n",
    "        self.wa = self.add_weight(name = 'wa', shape = (1, input_shape[1], self.k), initializer = 'uniform', trainable = True)\n",
    "        self.ba = self.add_weight(name = 'ba', shape = (1, input_shape[1], self.k), initializer = 'uniform', trainable = True)\n",
    "        super(Time2Vec, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        bias = self.wb * inputs + self.bb\n",
    "        dp = K.dot(inputs, self.wa) + self.ba\n",
    "        wgts = K.sin(dp)\n",
    "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
    "        ret = K.reshape(ret, (-1, inputs.shape[1] * (self.k + 1)))\n",
    "        return ret\n",
    "    def compute_output_shape(self, input_shape): return (input_shape[0], input_shape[1] * (self.k + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8693c9",
   "metadata": {
    "_cell_guid": "f793ec47-95c3-402e-9b1e-9ed28a58a10c",
    "_uuid": "3e51c37e-bf38-49ec-8bfe-79f2dfce8b4c",
    "papermill": {
     "duration": 0.02891,
     "end_time": "2022-01-13T07:10:36.018815",
     "exception": false,
     "start_time": "2022-01-13T07:10:35.989905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> ### See training notebook [here](https://www.kaggle.com/yamqwe/tutorial-time-series-transformer-time2vec)\n",
    "\n",
    "# <span class=\"title-section w3-xxlarge\" id=\"putting\">Putting It All Together 🏗️</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0628c",
   "metadata": {
    "_cell_guid": "43bb680d-0ff5-4447-a69f-9b543ca497b7",
    "_uuid": "d1fb6cb9-3efd-419e-8a1c-a2fc646c66a9",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:36.089223Z",
     "iopub.status.busy": "2022-01-13T07:10:36.088443Z",
     "iopub.status.idle": "2022-01-13T07:10:36.092252Z",
     "shell.execute_reply": "2022-01-13T07:10:36.093223Z",
     "shell.execute_reply.started": "2022-01-12T09:44:40.190565Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.043078,
     "end_time": "2022-01-13T07:10:36.093417",
     "exception": false,
     "start_time": "2022-01-13T07:10:36.050339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(input_shape, time2vec_dim = 3):\n",
    "    inp = Input(input_shape)\n",
    "    x = inp\n",
    "\n",
    "    time_embedding = keras.layers.TimeDistributed(Time2Vec(time2vec_dim - 1))(x)\n",
    "    x = Concatenate(axis = -1)([x, time_embedding])\n",
    "    x = layers.LayerNormalization(epsilon = 1e-6)(x)\n",
    "    x = layers.LayerNormalization(epsilon = 1e-6)(x)\n",
    "\n",
    "    for k in range(1):\n",
    "        x_old = x\n",
    "        transformer_block = TransformerBlock(64, input_shape[-1] + ( input_shape[-1] * time2vec_dim), 8, 4096, 0.0)\n",
    "        x = transformer_block(x)\n",
    "        x = ((1.0 - 0.2) * x) + (0.2 * x_old)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation = \"selu\")(x)\n",
    "    x = layers.Dropout(0.0)(x)\n",
    "    x = Dense(1, activation = 'linear')(x)\n",
    "\n",
    "    out = x\n",
    "    model = Model(inp, out)\n",
    "    model.compile(tf.keras.optimizers.Adam(LR), 'mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2c8ab",
   "metadata": {
    "_cell_guid": "a911e0b0-e1ac-4ca2-845a-8592808ab09d",
    "_kg_hide-input": true,
    "_uuid": "3c326434-3903-4e47-ad63-74b864fe081f",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:36.155415Z",
     "iopub.status.busy": "2022-01-13T07:10:36.154660Z",
     "iopub.status.idle": "2022-01-13T07:10:38.509451Z",
     "shell.execute_reply": "2022-01-13T07:10:38.509891Z",
     "shell.execute_reply.started": "2022-01-12T09:44:40.203854Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.392314,
     "end_time": "2022-01-13T07:10:38.510067",
     "exception": false,
     "start_time": "2022-01-13T07:10:36.117753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = engineer(original_train_df)\n",
    "train_df['date'] = original_train_df.date\n",
    "train_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\n",
    "test_df = engineer(original_test_df)\n",
    "features = test_df.columns\n",
    "for df in [train_df, test_df]: df[features] = df[features].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9d81c",
   "metadata": {
    "_cell_guid": "af4bdbe3-abda-4fa5-8f15-377229743d07",
    "_uuid": "2b7507b1-4302-43c9-a5f2-bf37a029b3b0",
    "papermill": {
     "duration": 0.016011,
     "end_time": "2022-01-13T07:10:38.542959",
     "exception": false,
     "start_time": "2022-01-13T07:10:38.526948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"training\">Inference 🔥</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cfe39f",
   "metadata": {
    "_cell_guid": "7990bb4b-b784-417f-8657-4905c6380aa2",
    "_uuid": "009be53f-8eac-445d-bddc-8c53fee23c06",
    "papermill": {
     "duration": 0.016024,
     "end_time": "2022-01-13T07:10:38.575090",
     "exception": false,
     "start_time": "2022-01-13T07:10:38.559066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Inference Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bcbddf",
   "metadata": {
    "_cell_guid": "27fc888c-4092-4db7-a5ec-a626a777ce9d",
    "_uuid": "809dfef0-0969-413f-ba94-c34aaf2ad709",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:38.610444Z",
     "iopub.status.busy": "2022-01-13T07:10:38.609598Z",
     "iopub.status.idle": "2022-01-13T07:10:38.627261Z",
     "shell.execute_reply": "2022-01-13T07:10:38.626851Z",
     "shell.execute_reply.started": "2022-01-12T09:44:42.487903Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.03627,
     "end_time": "2022-01-13T07:10:38.627369",
     "exception": false,
     "start_time": "2022-01-13T07:10:38.591099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_model(fold_idx, train_df, all_data):\n",
    "    if os.path.exists(f'./ts-transformer-trained/ts_transformer_100_epochs_fold_{fold_idx}_model_recreate_3.h5'):\n",
    "        preproc = StandardScaler()\n",
    "        x_all = pd.DataFrame(preproc.fit_transform(all_data[features]), columns = features)\n",
    "        all_data_ts = rolling_window(x_all, window_size = WINDOW_SIZE)\n",
    "        model = get_model(all_data_ts.shape[1:], 3)\n",
    "        model.load_weights(f'./ts-transformer-trained/ts_transformer_100_epochs_fold_{fold_idx}_model_recreate_3.h5')\n",
    "        preds = model.predict(all_data_ts)\n",
    "        test_preds = np.exp(preds[len(train_df) - WINDOW_SIZE:])\n",
    "        return test_preds\n",
    "    return None\n",
    "\n",
    "def engineer_more(df):\n",
    "    new_df = engineer(df)\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({f\"dec{d}\": (df.date.dt.month == 12) & (df.date.dt.day == d) for d in range(24, 32)})], axis = 1)\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({f\"jan{d}\": (df.date.dt.month == 1) & (df.date.dt.day == d) for d in range(1, 13)})], axis = 1)\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({f\"may{d}\": (df.date.dt.month == 5) & (df.date.dt.day == d) for d in list(range(1, 10)) + list(range(17, 25))})], axis = 1)\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({f\"june{d}\": (df.date.dt.month == 6) & (df.date.dt.day == d) for d in list(range(6, 14))})], axis = 1)\n",
    "    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')), 2016: pd.Timestamp(('2016-06-29')), 2017: pd.Timestamp(('2017-06-28')), 2018: pd.Timestamp(('2018-06-27')), 2019: pd.Timestamp(('2019-06-26'))})\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({f\"wed_june{d}\": (df.date - wed_june_date == np.timedelta64(d, \"D\")) for d in list(range(-5, 6))})], axis = 1)\n",
    "    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')), 2016: pd.Timestamp(('2016-11-6')), 2017: pd.Timestamp(('2017-11-5')), 2018: pd.Timestamp(('2018-11-4')), 2019: pd.Timestamp(('2019-11-3'))})\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({f\"sun_nov{d}\": (df.date - sun_nov_date == np.timedelta64(d, \"D\")) for d in list(range(0, 10))})], axis = 1)\n",
    "    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n",
    "    new_df = pd.concat([new_df, pd.DataFrame({f\"easter{d}\": (df.date - easter_date == np.timedelta64(d, \"D\")) for d in list(range(0, 9)) + list(range(50, 60)) + list(range(40, 46))})], axis = 1)\n",
    "    return new_df.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18393e1f",
   "metadata": {
    "_cell_guid": "2aac5991-a535-45d4-919c-0ac80f91e3d5",
    "_uuid": "f31fea39-1f31-4481-8fcd-e85302e39905",
    "papermill": {
     "duration": 0.016073,
     "end_time": "2022-01-13T07:10:38.659621",
     "exception": false,
     "start_time": "2022-01-13T07:10:38.643548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26ccf1",
   "metadata": {
    "_cell_guid": "5943a839-52b9-429d-ac89-26d8ca011f60",
    "_uuid": "c3cd657e-417a-463b-8d91-d5a083ce934c",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:10:38.698776Z",
     "iopub.status.busy": "2022-01-13T07:10:38.697988Z",
     "iopub.status.idle": "2022-01-13T07:12:24.150166Z",
     "shell.execute_reply": "2022-01-13T07:12:24.150628Z",
     "shell.execute_reply.started": "2022-01-12T09:44:42.51034Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 105.47504,
     "end_time": "2022-01-13T07:12:24.150849",
     "exception": false,
     "start_time": "2022-01-13T07:10:38.675809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = engineer_more(original_train_df)\n",
    "train_df['date'] = original_train_df.date\n",
    "train_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\n",
    "test_df = engineer_more(original_test_df)\n",
    "test_df.year = 2018\n",
    "features = test_df.columns\n",
    "\n",
    "RUNS = 10\n",
    "LOSS_CORRECTION = 1\n",
    "TRAIN_VAL_CUT = datetime(2018, 1, 1)\n",
    "np.random.seed(202100)\n",
    "\n",
    "test_pred_list = []\n",
    "all_data = pd.concat([train_df, test_df])\n",
    "total_start_time = datetime.now()\n",
    "for run in range(RUNS):\n",
    "    test_preds = predict_model(run, train_df, all_data)    \n",
    "    if test_preds is not None: test_pred_list.append(test_preds * LOSS_CORRECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd6676",
   "metadata": {
    "_cell_guid": "3c398104-3113-4b06-9754-a9e4d7c4c2a6",
    "_uuid": "4c38e114-d753-4198-aa35-e96544d7f30c",
    "papermill": {
     "duration": 0.015984,
     "end_time": "2022-01-13T07:12:24.184118",
     "exception": false,
     "start_time": "2022-01-13T07:12:24.168134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"submit\">Blending & Submitting To Kaggle 🇰</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c9859",
   "metadata": {
    "_cell_guid": "b413ac16-2f18-47f7-9862-c26e9e8dee46",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "ed19690a-6cf4-42e4-afc7-aabefb79020c",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:12:24.227882Z",
     "iopub.status.busy": "2022-01-13T07:12:24.227043Z",
     "iopub.status.idle": "2022-01-13T07:12:25.394688Z",
     "shell.execute_reply": "2022-01-13T07:12:25.395129Z",
     "shell.execute_reply.started": "2022-01-12T09:58:12.730607Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.194779,
     "end_time": "2022-01-13T07:12:25.395274",
     "exception": false,
     "start_time": "2022-01-13T07:12:24.200495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = original_test_df[['row_id']].copy()\n",
    "sub['num_sold'] = np.mean(np.concatenate([np.expand_dims(i, axis = 0) for i in test_pred_list], axis = 0), axis = 0)\n",
    "\n",
    "sub_blend_1 = pd.read_csv('./tps-01-2022/submission.csv')['num_sold'].values\n",
    "sub_blend_2 = pd.read_csv('./tell-me-the-magic-number/submission.csv')['num_sold'].values\n",
    "sub_blend_3 = pd.read_csv('./tpsjan22-03-linear-model/submission_linear_model_rounded.csv')['num_sold'].values\n",
    "\n",
    "sub['num_sold'] = (sub_blend_1 * 0.8) + (sub_blend_2 * 0.2)\n",
    "sub.to_csv('submission_updated.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(16,3))\n",
    "plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201), density=True, label='Training')\n",
    "plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201), density=True, rwidth=0.5, label='Test predictions')\n",
    "plt.xlabel('num_sold')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d37b44",
   "metadata": {
    "_cell_guid": "29fb2f35-0077-43aa-b2d3-a1369c92d930",
    "_uuid": "72313f37-a044-4a05-acff-15d829b6c2bd",
    "papermill": {
     "duration": 0.017234,
     "end_time": "2022-01-13T07:12:25.430237",
     "exception": false,
     "start_time": "2022-01-13T07:12:25.413003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Some Rounding Post Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e365b",
   "metadata": {
    "_cell_guid": "f1cde54e-d4c7-403a-8f51-0fce138aebc7",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "c87cc455-d6e8-41d4-a6a2-cd4a959d97cc",
    "execution": {
     "iopub.execute_input": "2022-01-13T07:12:25.471863Z",
     "iopub.status.busy": "2022-01-13T07:12:25.471112Z",
     "iopub.status.idle": "2022-01-13T07:12:26.914616Z",
     "shell.execute_reply": "2022-01-13T07:12:26.915027Z",
     "shell.execute_reply.started": "2022-01-12T09:58:31.458506Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.467752,
     "end_time": "2022-01-13T07:12:26.915170",
     "exception": false,
     "start_time": "2022-01-13T07:12:25.447418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_rounded = sub.copy()\n",
    "sub_rounded['num_sold'] = sub_rounded['num_sold'].round()\n",
    "sub_rounded.to_csv('submission2.csv', index=False)\n",
    "plt.figure(figsize=(16,3))\n",
    "plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201), density=True, label='Training')\n",
    "plt.hist(sub_rounded['num_sold'], bins=np.linspace(0, 3000, 201), density=True, rwidth=0.5, label='Test predictions')\n",
    "plt.xlabel('num_sold')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "sub_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e1f2c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.32.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.2\n",
      "    Uninstalling numpy-1.21.2:\n",
      "      Successfully uninstalled numpy-1.21.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-3dpfev3g\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2adce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 128.411508,
   "end_time": "2022-01-13T07:12:30.091158",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-13T07:10:21.679650",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
